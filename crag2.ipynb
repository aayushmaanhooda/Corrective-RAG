{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35353e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, TypedDict\n",
    "import time\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d58f2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = (\n",
    "    PyPDFLoader(\"./docs/book1.pdf\").load() +\n",
    "    PyPDFLoader(\"./docs/book2.pdf\").load() +\n",
    "    PyPDFLoader(\"./docs/book3.pdf\").load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5311fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = RecursiveCharacterTextSplitter(chunk_size = 900, chunk_overlap = 150).split_documents(docs)\n",
    "\n",
    "for d in chunks:\n",
    "    d.page_content = d.page_content.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store  = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "retriver = vector_store.as_retriever(search_type=\"similarity\" , search_kwargs = {'k':4})\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o\")\n",
    "filter_llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",  # Fast, good for filtering\n",
    "    temperature=0,\n",
    "    format=\"json\"  # Forces JSON output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPPER_TH = 0.7\n",
    "LOWER_TH= 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    docs: List[Document]\n",
    "\n",
    "    good_docs: List[Document]\n",
    "    verdict: str\n",
    "    reason: str\n",
    "\n",
    "    strips: List[str]  \n",
    "    kept_strips: List[str] \n",
    "    refined_context: str \n",
    "\n",
    "    web_docs: List[Document]\n",
    "    \n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive(state):\n",
    "    q = state[\"question\"]\n",
    "    return {\n",
    "        \"docs\" : retriver.invoke(q)\n",
    "    }\n",
    "\n",
    "# SCORE BASED DOC EVALUATOR\n",
    "class DocEvalScore(BaseModel):\n",
    "    score: float\n",
    "    reason: str\n",
    "\n",
    "\n",
    "doc_eval_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "        \"You are a strict retrival evaluator for RAG.\\n\"\n",
    "        \"You will be given one retrived chunk and question.\\n\"\n",
    "        \"return a relevance score from [0.0, 1.0].\\n\"\n",
    "        \"1.0 -> chunk alone is sufficient to answer fully/mostly.\\n\"\n",
    "        \"0.0 -> chunk is irrelevant\\n\"\n",
    "        \"Be conservative with high scores.\\n\"\n",
    "        \"Also return short reason\\n\"\n",
    "        \"Output json only.\\n\"\n",
    "        ),\n",
    "        (\"human\", \"Question: {question}\\n\\nChunck:\\n{chunk}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "doc_eval_chain = doc_eval_prompt | llm.with_structured_output(DocEvalScore)\n",
    "\n",
    "def eval_each_doc_node(state: State) -> State:\n",
    "\n",
    "    q = state[\"question\"]\n",
    "    \n",
    "    scores: List[float] = []\n",
    "    reasons: List[str] = []\n",
    "    good: List[Document] = []\n",
    "\n",
    "    for d in state[\"docs\"]:\n",
    "        out = doc_eval_chain.invoke({\"question\": q, \"chunk\": d.page_content})\n",
    "        scores.append(out.score)\n",
    "        reasons.append(out.reason)\n",
    "\n",
    "        # for CORRECT case we will refine only docs with score > LOWER_TH\n",
    "        if out.score > LOWER_TH:\n",
    "            good.append(d)\n",
    "\n",
    "    # 2) CORRECT if at least one doc > UPPER_TH\n",
    "    if any(s > UPPER_TH for s in scores):\n",
    "        return {\n",
    "            \"good_docs\": good,\n",
    "            \"verdict\": \"CORRECT\",\n",
    "            \"reason\": f\"At least one retrieved chunk scored > {UPPER_TH}.\",\n",
    "        }\n",
    "\n",
    "    # 3) INCORRECT if all docs < LOWER_TH\n",
    "    if len(scores) > 0 and all(s < LOWER_TH for s in scores):\n",
    "        why = \"No chunk was sufficient.\"\n",
    "        return {\n",
    "            \"good_docs\": [],\n",
    "            \"verdict\": \"INCORRECT\",\n",
    "            \"reason\": f\"All retrieved chunks scored < {LOWER_TH}. {why}\",\n",
    "        }\n",
    "\n",
    "    # 4) Anything in between => AMBIGUOUS\n",
    "    why = \"Mixed relevance signals.\"\n",
    "    return {\n",
    "        \"good_docs\": good,\n",
    "        \"verdict\": \"AMBIGUOUS\",\n",
    "        \"reason\": f\"No chunk scored > {UPPER_TH}, but not all were < {LOWER_TH}. {why}\",\n",
    "    }\n",
    "\n",
    "def decompose_to_sentences(text: str) -> List[str]:\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "\n",
    "    return [\n",
    "        s.strip() for s in sentences if len(s.strip()) > 20\n",
    "    ]\n",
    "\n",
    "\n",
    "class KeepOrDrop(BaseModel):\n",
    "    keep: bool\n",
    "\n",
    "filter_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "        \"You are a strict relevance filter.\\n\"\n",
    "        \"Return keep=true only if the sentence direclty helps answer the question.\\n\"\n",
    "        \"Use Only the sentence. Output JSON only.\\n\"\n",
    "        ),\n",
    "        (\"human\", \"Question: {question}\\n\\nSentence:\\n{sentence}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "filter_chain = filter_prompt | filter_llm.with_structured_output(KeepOrDrop)\n",
    "\n",
    "\n",
    "def refine(state: State) -> State:\n",
    "    q = state[\"question\"]\n",
    "\n",
    "    # combine retrieved docs into one context string\n",
    "    if state.get(\"verdict\") == \"CORRECT\":\n",
    "        context = \"\\n\\n\".join(d.page_content for d in state[\"good_docs\"]).strip()\n",
    "    else:\n",
    "        context = \"\\n\\n\".join(d.page_content for d in state[\"web_docs\"]).strip()\n",
    "\n",
    "\n",
    "    # 1. decomposition (context -> sentence strips)\n",
    "    strips = decompose_to_sentences(context)\n",
    "\n",
    "    # 2 filter: Keep only relvant strips\n",
    "    kept: List[str] = []\n",
    "\n",
    "    for s in strips:\n",
    "        if filter_chain.invoke({\"question\": q, \"sentence\":s}).keep:\n",
    "            kept.append(s)\n",
    "    \n",
    "    # 3 RECOMPOSE: glue kept strips back together (internal knowledge)\n",
    "    refined_context = \"\\n\".join(kept).strip()\n",
    "\n",
    "    return {\n",
    "        \"strips\": strips,\n",
    "        \"kept_strips\": kept,\n",
    "        \"refined_context\" : refined_context\n",
    "    }\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer from the context provided. If the context is empty or insufficient, say: 'I don't know.'\"),\n",
    "        (\"human\", \"Question: {question}\\n\\n Context:\\n{refined_context}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def generate(state: State) -> State:\n",
    "    out = (prompt | llm).invoke(\n",
    "        {\"question\": state[\"question\"], \"refined_context\": state[\"refined_context\"]}\n",
    "    )\n",
    "    return {\"answer\": out.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily = TavilySearchResults(max_results=5)\n",
    "\n",
    "def web_search_node(state: State) -> State:\n",
    "\n",
    "    q = state[\"question\"]  # no query rewrite\n",
    "    results = tavily.invoke({\"query\": q})  # no knowledge selection\n",
    "\n",
    "    web_docs = []\n",
    "    for r in results or []:\n",
    "\n",
    "        title = r.get(\"title\", \"\")\n",
    "        url = r.get(\"url\", \"\")\n",
    "        content = r.get(\"content\", \"\") or r.get(\"snippet\", \"\")\n",
    "        \n",
    "        text = f\"TITLE: {title}\\nURL: {url}\\nCONTENT:\\n{content}\"\n",
    "\n",
    "        web_docs.append(Document(page_content=text, metadata={\"url\": url, \"title\": title}))\n",
    "\n",
    "    return {\"web_docs\": web_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fail_node(state: State) -> State:\n",
    "#     return {\"answer\": f\"FAIL: {state['reason']}\"}\n",
    "\n",
    "def ambiguous_node(state: State) -> State:\n",
    "    return {\"answer\": f\"Ambiguous: {state['reason']}\"}\n",
    "\n",
    "def route_after_eval(state: State) -> str:\n",
    "    if state[\"verdict\"] == \"CORRECT\":\n",
    "        return \"refine\"\n",
    "    elif state[\"verdict\"] == \"INCORRECT\":\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"ambiguous\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba44e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = StateGraph(State)\n",
    "g.add_node(\"retrieve\", retreive)\n",
    "g.add_node(\"eval_each_doc\", eval_each_doc_node)\n",
    "g.add_node(\"refine\", refine)\n",
    "g.add_node(\"generate\", generate)\n",
    "g.add_node(\"web_search\", web_search_node)\n",
    "# g.add_node(\"fail\", fail_node)\n",
    "g.add_node(\"ambiguous\", ambiguous_node)\n",
    "\n",
    "\n",
    "g.add_edge(START, \"retrieve\")\n",
    "g.add_edge(\"retrieve\", \"eval_each_doc\")\n",
    "\n",
    "g.add_conditional_edges(\n",
    "    \"eval_each_doc\",\n",
    "    route_after_eval,\n",
    "    {\"refine\": \"refine\", \"web_search\": \"web_search\", \"ambiguous\": \"ambiguous\"}\n",
    ")\n",
    "g.add_edge(\"web_search\" ,\"refine\")\n",
    "g.add_edge(\"refine\", \"generate\")\n",
    "g.add_edge(\"generate\", END)\n",
    "g.add_edge(\"ambiguous\", END)\n",
    "\n",
    "app = g.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06696955",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = app.invoke(\n",
    "    {\n",
    "        \"question\": \"Top AI news from last week\",\n",
    "        \"docs\": [],\n",
    "        \"good_docs\": [],\n",
    "        \"verdict\": \"\",\n",
    "        \"reason\": \"\",\n",
    "        \"strips\": [],\n",
    "        \"kept_strips\": [],\n",
    "        \"refined_context\": \"\",\n",
    "        \"answer\": \"\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"VERDICT:\", res[\"verdict\"])\n",
    "print(\"REASON:\", res[\"reason\"])\n",
    "print(\"\\nOUTPUT:\\n\", res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias variance tradeoff\n",
    "# AI news from last week\n",
    "# What are attention mechanisms and why are they important in current models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
